# üíª Computer Organization and Architecture (COA) - Detailed Notes

## üìå Introduction to COA

### Computer Organization (CO)
* Refers to the way **hardware components are structured and interconnected** to achieve specific goals.
* Deals with the internal organization of components and how they work together to **execute instructions**.
* Organizational attributes include the design of:
    * **Registers**
    * **Data paths**
    * **Control units**
    * [[Memory Systems]]
    * [[Input/Output Interfaces]]
* Contributes to implementing the **architectural specification**.

### Computer Architecture (CA)
* Refers to the **design and basic structure** of a computer system, including hardware structures, their interconnections, and the principles guiding their organization.
* Concerned with optimizing the **performance** of a computer system and ensuring it executes instructions quickly and efficiently.
* Involves designing:
    * **Instruction Set Architecture (ISA):** defines the instructions a processor can execute.
    * [[Memory Hierarchy]]
    * **System interconnects**
    * [[Parallel Processing Techniques]]
    * **Performance optimization strategies**

---

## üÜö CA vs. CO Comparison

| Feature | Computer Architecture (CA) | Computer Organization (CO) |
|---|---|---|
| **Focus** | **'What'** (Functional behavior) | **'How'** (Structural relationship) |
| **Key Aspect** | Instruction Set Architecture (ISA) | Implementation |
| **Components** | **Logic components** | **Physical components** |

---

## üèõÔ∏è Basic Computer Organization
A computer system consists of five classic and functionally independent units:
[attachment_0](attachment)
* **Input Unit**
* **Memory Unit (Main Memory):** Stores data.
* **Central Processing Unit (CPU) / Processor:** Controls the operation and performs data processing. It contains:
    * **Control Unit**
    * **Arithmetic Logic Unit (ALU)**
* **Output Unit**
* **Control Unit (Part of CPU):** Directs the flow of data and instructions.
* **I/O (Input/Output):** Moves data between the computer and the external world.
* **System Interconnection:** Provides communication among the CPU, memory, and I/O through a **system bus**, keeping components attached.

---

## ‚è≥ History and Trends

### Generations of Computers
* **1st Gen:** Vacuum Tubes (e.g., **ENIAC**)
* **2nd Gen:** **Transistors** (Improved size, speed, and reliability)
* **3rd Gen:** **Integrated Circuits (IC)**

### Technology and Performance Trend (Relative performance/unit cost)
* 1951: Vacuum tube (1)
* 1965: Transistor (35)
* 1975: Integrated circuit (900)
* 1995: Very large-scale integrated circuit (2,400,000)
* 2003: Ultra large-scale integrated circuit (250,000,000,000)

### Moore's Law
* States that the **number of transistors on a microchip doubles approximately every two years**.
* This leads to an **exponential increase in computational power** while reducing the cost per transistor.
* **Threats to Moore's Law** (Gradual decline):
    * Physical Limitations
    * **Heat Dissipation**
    * Manufacturing Costs
    * Energy Efficiency

### Other Exponential Changes
* **Computational Capacity:** Doubling every **1.5 years**.
* **Computing Efficiency and Cost:** Computing efficiency (energy use) has **halved every 1.5 years** over the last 60 years.

---

## ‚ú® The Eight Great Ideas in Computer Architecture

1.  **Design for Moore‚Äôs Law**
    * Architects must anticipate where the technology will be when the design finishes.
2.  **Use Abstraction to Simplify Design**
    * Represent the design at different levels to manage complexity and keep up with Moore's law's growth.
3.  **Make the Common Case Fast**
    * Leads to better overall performance and responsiveness in real-world scenarios. (Related to [[Amdahl's Law]])
4.  **Performance via Parallelism**
    * Promotes **parallel processing** for higher throughput and improved execution times.
5.  **Performance via Pipelining**
    * Breaks down instruction execution into stages and **overlaps them** to improve throughput.
6.  **Performance via Prediction**
    * Guess and start working rather than waiting, assuming recovery from misprediction is not too expensive and the prediction is accurate (e.g., [[Branch Prediction]]).
7.  **Hierarchy of Memories**
    * Uses multiple levels of memory (registers, caches, main memory) to provide a **trade-off between speed, capacity, and cost**.
8.  **Dependability via Redundancy**
    * Includes **redundant components** to take over when a failure occurs and to help detect failures (e.g., [[RAID]]).

---

## ‚ö° The Power Wall

* **Definition:** A point beyond which increasing a processor's clock speed or overall performance becomes extremely difficult due to constraints from **power consumption and heat dissipation**.
* **Dynamic Power Formula:**
    $$P \approx C \times V^2 \times f$$
    * Where $P$ is power, $C$ is capacitance, $V$ is voltage, and $f$ is frequency (clock rate).
    * Historically, reducing voltage $(V)$ helped lower power consumption.
* **Issues Limiting Further Voltage Reduction:**
    * **Current Leakage**
    * Expensive cooling technology to deal with heat dissipation
* **Solution to the Power Wall:**
    * The challenge prompted a shift in processor design.
    * A key example is the switch from Uniprocessors to **Multiprocessors (Multi-core CPUs)**.
    * **Advantages of Multiprocessors:** Improved performance, increased throughput, energy efficiency, and fault tolerance.
    * **Requirement for Programmers:** Must design or rewrite code to effectively use and scale performance with the increasing number of cores (i.e., requires **parallel programming**).

---

## ‚ùå Fallacies and Pitfalls

### Fallacies
* **Fallacy 1: Computers at low utilization use little power**
    * The relationship is complex; power consumption includes fixed overhead, peripheral devices, and maintenance.
    * A computer at **10% load can still use 33% of its peak power**.
* **Fallacy 2: Designing for performance and designing for energy efficiency are unrelated goals**
    * Optimizations that take less time **save energy overall**, even if the optimization itself takes slightly more energy when used.

### Pitfalls
* **Pitfall 1: Expecting the improvement of one aspect of a computer to increase overall performance by an amount proportional to the size of the improvement**
    * Enhancing a single component does **not** lead to a linear improvement in overall performance because other components may not keep up.
    * [[Amdahl's Law]] illustrates this: Performance enhancement is **limited by the amount the improved feature is actually used**.
* **Pitfall 2: Using a subset of the performance equation as a performance metric**
    * Solely using a subset (e.g., just Clock rate) can lead to an incomplete or misleading understanding of overall performance.
    * **Performance depends on:** Clock rate, Instruction count, and **CPI (Cycles Per Instruction)**.
    * Other factors like Latency, Throughput, and power consumption are also considered. A higher clock speed does not guarantee better performance if the other processor has a lower CPI or better instruction-level parallelism.
