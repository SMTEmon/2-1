***
**Lecturer:** [[Gilbert Strang]]
**Source:** MIT OpenCourseWare
**Tags:** #LinearAlgebra #Mathematics #Matrices #GaussianElimination #LUFactorization

---

## 1. Inverse of a Product
How do we find the inverse of a matrix product, $(AB)^{-1}$?
If we know $A$ and $B$ are invertible, we apply the operations in **reverse order**.

$$ (AB)^{-1} = B^{-1}A^{-1} $$

**Proof:**
$$ (AB) (B^{-1}A^{-1}) = A(BB^{-1})A^{-1} = A(I)A^{-1} = AA^{-1} = I $$

---

## 2. Inverse of a Transpose
A major property used in future lectures: The inverse o inf a transpose is the transpose of the inverse. The order of operations does not matter.

$$ (A^T)^{-1} = (A^{-1})^T $$

**Derivation:**
Start with $A A^{-1} = I$.
Transpose both sides (remembering that for products, transposing reverses the order):
$$ (A^{-1})^T A^T = I^T = I $$
Therefore, $(A^{-1})^T$ must be the inverse of $A^T$.

---

## 3. A = LU Factorization (The Core Concept)
This is the matrix description of **Gaussian Elimination**. Ideally, we want to factor matrix $A$ into a Lower triangular matrix ($L$) and an Upper triangular matrix ($U$).

*Assumption:* No row exchanges are required (pivots are non-zero).

### 2x2 Example
Let Matrix $A = \begin{bmatrix} 2 & 1 \\ 8 & 7 \end{bmatrix}$.

**Step 1: Elimination**
We need to eliminate the $8$ in position $(2,1)$.
*   Pivot: $2$
*   Multiplier: $4$ ($2 \times 4 = 8$)
*   Operation: Row 2 - 4(Row 1).

The **Elementary Matrix** ($E_{21}$) that performs this subtraction is:
$$ E_{21} = \begin{bmatrix} 1 & 0 \\ -4 & 1 \end{bmatrix} $$

Applying this to $A$:
$$ E_{21}A = \begin{bmatrix} 1 & 0 \\ -4 & 1 \end{bmatrix} \begin{bmatrix} 2 & 1 \\ 8 & 7 \end{bmatrix} = \begin{bmatrix} 2 & 1 \\ 0 & 3 \end{bmatrix} = U $$
Here, $U$ is the **Upper Triangular** result of elimination.

**Step 2: Factorization (Inverting the Operation)**
We want to express $A$ in terms of $U$. We move $E_{21}$ to the other side by using its inverse.
$$ A = E_{21}^{-1} U $$
$$ A = L U $$

The inverse of subtracting 4 times row 1 is **adding 4 times row 1**.
$$ L = E_{21}^{-1} = \begin{bmatrix} 1 & 0 \\ \mathbf{4} & 1 \end{bmatrix} $$

> [!tip] Key Insight
> The multiplier used in elimination (4) shows up directly in the $L$ matrix at the exact position $(2,1)$ where we eliminated the entry.

### The LDU Factorization (Variation)
Sometimes we want to separate the pivots from $U$. We can split $U$ into a diagonal matrix $D$ (containing the pivots) and a new $U$ (with 1s on the diagonal).
$$ A = L D U $$
$$ \begin{bmatrix} 2 & 1 \\ 8 & 7 \end{bmatrix} = \begin{bmatrix} 1 & 0 \\ 4 & 1 \end{bmatrix} \begin{bmatrix} 2 & 0 \\ 0 & 3 \end{bmatrix} \begin{bmatrix} 1 & 1/2 \\ 0 & 1 \end{bmatrix} $$

---

## 4. The 3x3 Case (Why L is better than E)
For a 3x3 matrix, elimination usually involves three steps (matrices $E_{21}$, $E_{31}$, $E_{32}$).

**The Elimination View (Messy):**
$$ E_{32} E_{31} E_{21} A = U $$
If we multiply all the $E$'s together to get one matrix $E$, the numbers mix up and become complicated. The history of operations is tangled.

**The Factorization View (Clean):**
We move the $E$'s to the right side by inverting them. Note the order reverses.
$$ A = (E_{21}^{-1} E_{31}^{-1} E_{32}^{-1}) U $$
$$ A = L U $$

**Visualizing L:**
If no row exchanges are needed, $L$ is formed by placing the **multipliers** directly into their respective positions below the diagonal.

$$ L = \begin{bmatrix} 1 & 0 & 0 \\ l_{21} & 1 & 0 \\ l_{31} & l_{32} & 1 \end{bmatrix} $$

> [!abstract] Deep Dive: The "Magic" of L vs E (Concrete Example)
> This is a subtle point. Matrix multiplication usually implies mixing, but $L$ keeps multipliers distinct while $E$ mixes them.
>
> **1. The "Messy" Way ($E$):**
> Imagine subtracting **2** times Row 1 from Row 2, then **5** times Row 2 from Row 3.
> $$ E_{total} = E_{step2} \times E_{step1} = \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & \mathbf{-5} & 1 \end{bmatrix} \begin{bmatrix} 1 & 0 & 0 \\ \mathbf{-2} & 1 & 0 \\ 0 & 0 & 1 \end{bmatrix} = \begin{bmatrix} 1 & 0 & 0 \\ -2 & 1 & 0 \\ \mathbf{10} & -5 & 1 \end{bmatrix} $$
> *Where did the 10 come from?* Row 3 is affected by Row 1 *indirectly* through Row 2. The operations compound ($(-2) \times (-5) = 10$). The history is tangled.
>
> **2. The "Clean" Way ($L$):**
> $L$ reverses the order and inverts the signs to **reconstruct** $A$.
> $$ L = E_{step1}^{-1} \times E_{step2}^{-1} = \begin{bmatrix} 1 & 0 & 0 \\ \mathbf{2} & 1 & 0 \\ 0 & 0 & 1 \end{bmatrix} \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & \mathbf{5} & 1 \end{bmatrix} = \begin{bmatrix} 1 & 0 & 0 \\ \mathbf{2} & 1 & 0 \\ 0 & \mathbf{5} & 1 \end{bmatrix} $$
> *Where is the 10?* It's gone! The multipliers **2** and **5** sit purely in their positions ($l_{21}$ and $l_{32}$).
>
> **Intuition:**
> *   **$E$ (Elimination)** records the **process** (affected by changing rows).
> *   **$L$ (Reconstruction)** records the **instruction set** ("Row 3 needed 5 of Row 2").

---

## 5. Computational Complexity (Operation Counts)
How expensive is elimination for an $n \times n$ matrix?
*Operation definition:* 1 Multiply + 1 Subtract.

### Cost for Matrix A (Left Side)
*   **Step 1:** The first pivot row modifies $n-1$ rows below it. Each row has $n$ elements (approx). Operations $\approx n^2$.
*   **Step 2:** The next pivot modifies $n-2$ rows. Operations $\approx (n-1)^2$.
*   **Sum:** $n^2 + (n-1)^2 + \dots + 1^2 \approx \sum_{i=1}^n i^2$.

Using the calculus approximation ($\int x^2 dx = \frac{1}{3}x^3$):
$$ \text{Operations for A} \approx \frac{1}{3}n^3 $$

### Cost for Vector b (Right Side)
We also have to process the right-hand side vector $b$.
*   Operations $\approx n^2$.

> [!important] Practical Implication
> For very large $n$ (e.g., $n=1000$):
> *   Factorization ($A=LU$) is expensive ($\frac{1}{3}n^3$).
> *   Solving for specific solutions ($b$) is cheap ($n^2$).
> Once you pay the cost to find $L$ and $U$, you can solve for many different $b$'s very quickly.

---

## 6. Permutations (Row Exchanges)
If a 0 appears in a pivot position, we must swap that row with a row below it.
*   **Permutation Matrix ($P$):** The identity matrix with rows reordered.
*   To swap rows $i$ and $j$ of matrix $A$, compute $PA$.

**Properties of Permutations:**
1.  There are $n!$ permutation matrices of size $n \times n$.
2.  They form a **Group**:
    *   Product of two P's is a P.
    *   Inverses exist (and are P's).
3.  **Crucial Property:** The inverse is the transpose.
    $$ P^{-1} = P^T $$

**General Factorization:**
If row exchanges are required, the factorization becomes:
$$ PA = LU $$
*(Where P is the permutation matrix that puts the rows in the correct order for elimination)*.