**Lecturer:** Gilbert Strang (MIT)
**Topic:** Matrix Multiplication rules, Inverse Matrices, and Gauss-Jordan Elimination.
**Tags:** #LinearAlgebra #MIT1806 #Matrices #GaussJordan

---

## 1. Matrix Multiplication (4 Ways)

Given matrix $A$ of shape $(m \times n)$ and matrix $B$ of shape $(n \times p)$, the product $C = AB$ will have shape $(m \times p)$.

> [!WARNING] Rule of Shapes
> The number of **columns** in $A$ ($n$) must equal the number of **rows** in $B$ ($n$).

### Way 1: The Standard Way (Row $\cdot$ Column)
This is the standard entry-by-entry calculation using the dot product.
To find the entry in the $i$-th row and $j$-th column of $C$ (denoted $C_{ij}$):

$$C_{ij} = (\text{row } i \text{ of } A) \cdot (\text{column } j \text{ of } B)$$

**Formula:**
$$C_{ij} = \sum_{k=1}^{n} a_{ik} b_{kj}$$

### Way 2: The Column Way (Combinations of Columns)
This view is critical for understanding Linear Algebra conceptually.
*   The columns of $C$ are **linear combinations** of the columns of $A$.
*   The coefficients for these combinations come from the columns of $B$.

$$
\begin{bmatrix} \\ A \\ \\ \end{bmatrix}
\begin{bmatrix} | \\ \vec{b}_j \\ | \end{bmatrix}
=
\begin{bmatrix} | \\ \vec{c}_j \\ | \end{bmatrix}
$$

**Formula:**
$$ \text{Col } j \text{ of } C = A \times (\text{Col } j \text{ of } B) $$

> [!TIP] Insight
> If you multiply matrix $A$ by a single column vector $x$, the result is a linear combination of A's columns. Therefore, multiplying $A$ by matrix $B$ is just doing this for *every* column in $B$ side-by-side.

### Way 3: The Row Way (Combinations of Rows)
This is the dual of the column way.
*   The rows of $C$ are **linear combinations** of the rows of $B$.
*   The coefficients come from the rows of $A$.

$$
\begin{bmatrix} - & \vec{a}_i & - \end{bmatrix}
\begin{bmatrix} & B & \\ & & \end{bmatrix}
=
\begin{bmatrix} - & \vec{c}_i & - \end{bmatrix}
$$

**Formula:**
$$ \text{Row } i \text{ of } C = (\text{Row } i \text{ of } A) \times B $$

### Way 4: Column $\times$ Row (Sum of Outer Products)
This is less common in hand calculations but vital for theory.
Multiplying a single column $(m \times 1)$ by a single row $(1 \times p)$ creates a full matrix $(m \times p)$.

$$AB = \text{Sum of } (\text{Col } k \text{ of } A) \times (\text{Row } k \text{ of } B)$$

$$AB = \sum_{k=1}^{n} (\vec{col}_k(A))(\vec{row}_k(B))$$

---

## 2. Block Multiplication
If matrices are partitioned into blocks (sub-matrices), and the shapes of those blocks match up correctly, you can multiply the blocks as if they were scalars.

**Example:**
$$
\begin{bmatrix} A_1 & A_2 \\ A_3 & A_4 \end{bmatrix}
\begin{bmatrix} B_1 & B_2 \\ B_3 & B_4 \end{bmatrix}
=
\begin{bmatrix} C_{11} & C_{12} \\ C_{21} & C_{22} \end{bmatrix}
$$

Where:
$$C_{11} = A_1 B_1 + A_2 B_3$$
(and so on for other blocks, provided inner dimensions allow multiplication).

---

## 3. Inverses (Square Matrices)

We focus on square matrices ($n \times n$).

**Definition:** The inverse of matrix $A$, denoted $A^{-1}$, satisfies:
$$A^{-1} A = I \quad \text{and} \quad A A^{-1} = I$$

### Terminology
*   **Invertible** or **Non-singular**: The inverse exists.
*   **Singular**: No inverse exists.

### The "No Inverse" Condition
How do we know if a matrix is singular (no inverse)?

> [!ERROR] Singular Condition
> A matrix $A$ has no inverse if there exists a non-zero vector $x$ such that $Ax = 0$

**Why?**
If $Ax = 0$ and $A^{-1}$ existed, we could multiply both sides by the inverse:
1. $A^{-1}(Ax) = A^{-1}(0)$
2. $(A^{-1}A)x = 0$
3. $Ix = 0$
4. $x = 0$
But we stated $x \neq 0$. Contradiction. Therefore, $A^{-1}$ cannot exist.

**Example of Singular Matrix:**
The columns are linearly dependent (they lie on the same line).
$$
A = \begin{bmatrix} 1 & 3 \\ 2 & 6 \end{bmatrix}
$$
We can find a non-zero $x$ that makes $Ax=0$:
$$
\begin{bmatrix} 1 & 3 \\ 2 & 6 \end{bmatrix}
\begin{bmatrix} 3 \\ -1 \end{bmatrix}
=
\begin{bmatrix} 0 \\ 0 \end{bmatrix}
$$
Because the output is zero, we cannot "invert" the operation to get back to the input.

Another way to think about this: 
The equation $Ax=0$ will have non zero solutions for any singular matrix. These vectors form the null space. Since there are multiple such solutions that lead to this outcome, it is not possible to trace back which transformation it was initially.

---

## 4. Gauss-Jordan Elimination (Finding $A^{-1}$)

How do we actually compute $A^{-1}$? We solve the equation $AA^{-1} = I$.
This is equivalent to solving systems of equations for $n$ different right-hand sides (the columns of the Identity matrix).

**The Setup (Augmented Matrix):**
We construct a long matrix $[ A \mid I ]$.

**The Process:**
We perform standard elimination (row operations) on $A$ to turn it into the Identity matrix $I$. Because we apply the operations to the *whole* augmented row, $I$ turns into $A^{-1}$.

$$
[ A \mid I ] \xrightarrow{\text{Row Operations}} [ I \mid A^{-1} ]
$$

### Example
Find the inverse of $A = \begin{bmatrix} 1 & 3 \\ 2 & 7 \end{bmatrix}$.

1.  **Set up augmented matrix:**
    $$
    \left[
    \begin{array}{cc|cc}
    1 & 3 & 1 & 0 \\
    2 & 7 & 0 & 1
    \end{array}
    \right]
    $$

2.  **Elimination (Downwards):** Subtract $2 \times$ Row 1 from Row 2.
    $$
    \left[
    \begin{array}{cc|cc}
    1 & 3 & 1 & 0 \\
    0 & 1 & -2 & 1
    \end{array}
    \right]
    $$

3.  **Elimination (Upwards):** Subtract $3 \times$ Row 2 from Row 1.
    $$
    \left[
    \begin{array}{cc|cc}
    1 & 0 & 7 & -3 \\
    0 & 1 & -2 & 1
    \end{array}
    \right]
    $$

4.  **Result:**
    $$ A^{-1} = \begin{bmatrix} 7 & -3 \\ -2 & 1 \end{bmatrix} $$

### Why does Gauss-Jordan work?
We can view row operations as multiplying by elimination matrices ($E$).
1. We start with $[ A \mid I ]$.
2. We multiply by elimination matrices $E$ until the left side becomes $I$.
3. Effectively: $E [ A \mid I ] = [ EA \mid EI ]$.
4. If the elimination succeeds, $EA = I$.
5. By definition, if $EA = I$, then $E$ must be $A^{-1}$.
6. Therefore, the right side $EI = A^{-1}I = A^{-1}$.